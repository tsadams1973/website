<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Computer Vision using Azure Cognitive Services - iconic sections</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="Computer Vision using Azure Cognitive Services" />
<meta property="og:description" content="A quick introduction to recognizing analyzing the content in images using python and azure." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://timblog.z19.web.core.windows.net/posts/azure-ai-vision/" />
<meta property="article:published_time" content="2020-02-13T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2020-02-13T00:00:00&#43;00:00"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Computer Vision using Azure Cognitive Services"/>
<meta name="twitter:description" content="A quick introduction to recognizing analyzing the content in images using python and azure."/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:500,100,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" /><script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script><script src="/js/main.js"></script>
	<link rel="shortcut icon" href="/images/Covatrix_Icon.ico">
</head>

<body>
	<div class="container wrapper post">
		<div class="header">
	<h1 class="site-title">iconic sections</h1>
	<div class="site-description"><h2>discord and hyperbole</h2><nav class="nav social">
			<ul class="flat"><a href="https://github.com/tsadams1973" title="Github"><i data-feather="github"></i></a><a href="https://twitter.com/tsadams1973" title="Twitter"><i data-feather="twitter"></i></a><a href="/index.xml" title="RSS"><i data-feather="rss"></i></a></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">Home</a>
			</li>
			
			<li>
				<a href="/posts">All posts</a>
			</li>
			
			<li>
				<a href="/about">About</a>
			</li>
			
			<li>
				<a href="/resume">Resume</a>
			</li>
			
			<li>
				<a href="/tags">Tags</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">Computer Vision using Azure Cognitive Services</h1>
			<div class="meta">Posted on Feb 13, 2020</div>
		</div>

		<div class="markdown">
			

<p>As part of my effort to learn more about artificial intelligence, machine learning, and Azure, I have been reading <a href="https://www.apress.com/gp/book/9781484236789">Deep Learning with Azure</a>. Having just finished <strong>Part II: Azure AI Platform and Experimentation Tools</strong>, I thought I might write up my experiences working with <a href="https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/">computer vision</a> and <a href="https://azure.microsoft.com/en-us/services/cognitive-services/custom-vision-service/">custom vision</a> while they are still fresh in my mind.</p>

<hr />

<p>Despite the majority of my professional programming experience being in C#, I decided to use <a href="https://www.python.org/">Python</a> to work through the examples. Given its popularity in the data science and machine learning communities, it made sense to me to get some more practice with it. With <a href="https://code.visualstudio.com/">Visual Studio Code</a> being my preferred editor, the first step was to set it up for Python development.</p>

<h4 id="setting-up-visual-studio-code-for-python-development">Setting up Visual Studio Code for Python Development</h4>

<p>Assuming you already use VS Code, Microsoft provides a <a href="https://code.visualstudio.com/docs/python/python-tutorial">good tutorial</a> on how to get started using it with Python. Step one was to install the <a href="https://marketplace.visualstudio.com/items?itemName=ms-python.python">Python extension for VS Code</a>. I chose to use<a href="https://www.anaconda.com/distribution/">Anaconda</a> to manage my environments, so step two was to install it.</p>

<p>The step installing Anaconda is the main reason I&rsquo;m including this part. First, given that I use a Mac and have recently upgraded to <a href="https://www.apple.com/macos/catalina/">macOS Catalina</a>, I also started using <em><a href="https://github.com/ohmyzsh/ohmyzsh/wiki/Installing-ZSH">ZSH</a></em> as my default shell instead of <em>bash</em>. So, while Anaconda installed correctly, and the application worked, I had trouble with using <em>conda</em> in the terminal.  I received the error:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-zsh" data-lang="zsh">zsh: command not found: conda</code></pre></div>
<p>After some searching I found some help on-line. The two primary sources I used were this <a href="https://towardsdatascience.com/how-to-successfully-install-anaconda-on-a-mac-and-actually-get-it-to-work-53ce18025f97">Medium article</a> and this <a href="https://www.anaconda.com/how-to-restore-anaconda-after-macos-catalina-update/">post from the Anaconda team</a>. The solution that worked for me was to run this from the terminal:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-zsh" data-lang="zsh">eval <span style="color:#a31515">&#34;</span><span style="color:#00f">$(</span>/{your-path-here}/conda shell.zsh hook<span style="color:#00f">)</span><span style="color:#a31515">‚Äù
</span><span style="color:#a31515">conda init zsh</span></code></pre></div>
<p>After that, <em>conda</em> worked as expected from the command line. And now you should be ready to work with Python using VS Code, which means you are also ready to work through the computer vision examples.</p>

<h4 id="computer-vision">Computer Vision</h4>

<p>Computer Vision is one of the AI services found under Microsoft&rsquo;s Cognitive Services. It analyzes the content found in images. If you scroll down on <a href="https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/">this page</a> you can see it in action using a simple tool. You can easily create a new computer vision service from the <a href="https://portal.azure.com">Azure portal</a> (see <em>Figure 1</em> and <em>Figure 2</em> below).</p>










<figure> 
  <img style="max-width: 100%; width: auto; height: auto;" src="/posts/azure-ai-vision/azure-ai-1_hu21b508665d81e332317104bc519c645a_133895_600x0_resize_box_2.png" width="600" height="447" alt=" Computer Vision ">
  <figcaption>
      <small>
          <center>
              <strong>
    
	Figure 1
    
             </strong>
            </center>
      </small>
    </figcaption>
</figure>











<figure> 
  <img style="max-width: 100%; width: auto; height: auto;" src="/posts/azure-ai-vision/azure-ai-2_hueff89502a567eb2a8ca9db813f6deb9c_101025_600x0_resize_box_2.png" width="600" height="425" alt=" Computer Vision ">
  <figcaption>
      <small>
          <center>
              <strong>
    
	Figure 2
    
             </strong>
            </center>
      </small>
    </figcaption>
</figure>


<p>Once you create the service you can navigate to the <em>Quick start</em> tab under the <em>Resource Management</em> section of its blade to find some useful information. First, it will show you the <em>api key</em> and <em>endpoint</em> you will need to work with the service. Then it will show you two ways to try it out.</p>

<p>The first option is to navigate to the Cognitive Services <a href="https://westus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fa">API Console</a>. (Make sure to copy your <em>api key</em> and note the location that you created your service in, you will need them.) From the API Console you can click the region for your service and then submit calls to see how the api works and what it returns. If you need some images to submit, I found this site useful: <a href="https://free-images.com/">free-images.com</a>. Once you are familiar with how the API operates, you can go back to the <em>Quick starts</em> in the portal to learn how to work with it using the custom vision client library for Python.</p>

<p>Microsoft has created a <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/quickstarts-sdk/python-sdk">simple tutorial</a> to show how the computer vision client library works. It includes all of the code you need to set up your environment and analyze images. If you&rsquo;d rather just download the complete code, you find the official code on GitHub <a href="https://github.com/Azure-Samples/cognitive-services-quickstart-code/tree/master/python/ComputerVision">here</a>, or see my example <a href="https://github.com/tsadams1973/python/blob/master/quickstart-file.py">here</a>.</p>

<p>A few notes. First, since I was using <em>conda</em> to manage my environment, I also wanted to use that to set my environment variables. The code in the samples stays the same, but the <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cunix#configure-an-environment-variable-for-authentication">process to set up the variables</a> is different from what is listed in the tutorial. I found <a href="https://towardsdatascience.com/how-to-hide-your-api-keys-in-python-fb2e1a61b0a0">this guide</a> useful to set it up, or you can refer directly to the <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#macos-and-linux">conda documentation</a>. In addition to setting up the environment variables, I also had to <a href="https://pillow.readthedocs.io/en/stable/installation.html">install Pillow</a> in my environment. But after that, you should be able to use computer vision to analyze any images you would like.</p>

<h4 id="custom-vision">Custom Vision</h4>

<p>Custom Vision is also an AI Cognitive Service. It is an end-to-end platform built on computer vision that let&rsquo;s you create a train custom models tailored to your needs. Like the computer vision service, you can create this in the Azure portal as well (see <em>Figure 3</em>). Make sure to use the free tier if this is just for development and testing.</p>










<figure> 
  <img style="max-width: 100%; width: auto; height: auto;" src="/posts/azure-ai-vision/azure-ai-3_hu1a55aeaaceb91f3c5ea2cfd85be84521_137089_600x0_resize_box_2.png" width="600" height="473" alt=" Custom Vision ">
  <figcaption>
      <small>
          <center>
              <strong>
    
	Figure 3
    
             </strong>
            </center>
      </small>
    </figcaption>
</figure>


<p>After you create this, you will see two new services in your resource group: one for training and one for prediction. Opening either and looking at the <em>Quick start</em> option in its blade will give you some useful information to get started.</p>

<p>First thing to know is that custom vision includes its own <a href="https://www.customvision.ai/">portal</a> that you can use to create and train your own models visually. It is a good idea to go sign in to that portal. Although we will be using the Custom Vision Python SDK to create our project, we will use the portal to examine it afterwards.</p>

<p>You will find a link to a <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/python-tutorial">Quickstart: Create an image classification project with the Custom Vision Python SDK</a> tutorial. Like the previous tutorial, you can find all of the code on that page, but if you&rsquo;d like to, you can find the complete code that I used <a href="https://github.com/tsadams1973/python/blob/master/custom-vision.py">here</a>. (I only deviated from the code in the tutorial by including environment variables for the keys, endpoints, and paths as discussed above.) Another thing to note if you are running through the tutorial is that you will need to download the images from <a href="https://github.com/Azure-Samples/cognitive-services-python-sdk-samples/tree/master/samples/vision/images">this repo</a> to train the model. After that you should be able to run the sample code and see some output like that in <em>Figure 4</em>.</p>










<figure> 
  <img style="max-width: 100%; width: auto; height: auto;" src="/posts/azure-ai-vision/azure-ai-4_hu0f34f67766451d39b2a2c55a73dabbb0_47791_300x0_resize_box_2.png" width="300" height="184" alt=" Custom Vision ">
  <figcaption>
      <small>
          <center>
              <strong>
    
	Figure 4
    
             </strong>
            </center>
      </small>
    </figcaption>
</figure>


<p>After that you can go back to <a href="https://www.customvision.ai/">customvision.ai</a> to explore your project visually. You can see it in the portal as in <em>Figure 5</em> below, and then explore some of its properties (or retrain it, etc.) as in <em>Figure 6</em>.</p>










<figure> 
  <img style="max-width: 100%; width: auto; height: auto;" src="/posts/azure-ai-vision/azure-ai-5_hub188f5c814a6385a743d6bcbab3b9ba7_349863_600x0_resize_box_2.png" width="600" height="451" alt=" Custom Vision ">
  <figcaption>
      <small>
          <center>
              <strong>
    
	Figure 5
    
             </strong>
            </center>
      </small>
    </figcaption>
</figure>











<figure> 
  <img style="max-width: 100%; width: auto; height: auto;" src="/posts/azure-ai-vision/azure-ai-6_hu839408a8a5eac2e5ed27877ea75acbbc_285369_600x0_resize_box_2.png" width="600" height="329" alt=" Custom Vision ">
  <figcaption>
      <small>
          <center>
              <strong>
    
	Figure 6
    
             </strong>
            </center>
      </small>
    </figcaption>
</figure>


<hr />

<p>This is just the most basic introduction to using the Azure Vision Cognitive services, but hopefully it has helped you start to see what is possible and encouraged you to explore.</p>

		</div></div>
	<div class="footer wrapper">
	<nav class="nav">
		<div></div>
	</nav>
</div>

<script>feather.replace()</script>
</body>
</html>
